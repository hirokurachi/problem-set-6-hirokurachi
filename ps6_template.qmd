---
title: "Problem Set 6 - Waze Shiny Dashboard"
author: "Hiroaki Kurachi"
date: today
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---
1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. 

We use (`*`) to indicate a problem that we think might be time consuming. 

# Steps to submit (10 points on PS6) {-}

1. "This submission is my work alone and complies with the 30538 integrity
policy." Add your initials to indicate your agreement: \*\*\HK\*\*
2. "I have uploaded the names of anyone I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  \*\*\HK\*\* (2 point)
3. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*

4. Before starting the problem set, make sure to read and agree to the terms of data usage for the Waze data [here](https://canvas.uchicago.edu/courses/59054/quizzes/130617).

5. Knit your `ps6.qmd` as a pdf document and name it `ps6.pdf`.
6. Push your `ps6.qmd`, `ps6.pdf`, `requirements.txt`, and all created folders (we will create three Shiny apps so you will have at least three additional folders) to your Github repo (5 points). It is fine to use Github Desktop.
7. Submit `ps6.pdf` and also link your Github repo via Gradescope (5 points)
8. Tag your submission in Gradescope. For the Code Style part (10 points) please tag the whole correspondingsection for the code style rubric.

*Notes: see the [Quarto documentation (link)](https://quarto.org/docs/authoring/figures.html) for directions on inserting images into your knitted document.*

*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to "import" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*

```{python}
#| echo: true
#| eval: false

def print_file_contents(file_path):
    """Print contents of a file."""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
            print("```python")
            print(content)
            print("```")
    except FileNotFoundError:
        print("```python")
        print(f"Error: File '{file_path}' not found")
        print("```")
    except Exception as e:
        print("```python") 
        print(f"Error reading file: {e}")
        print("```")

print_file_contents("./top_alerts_map_byhour/app.py") # Change accordingly
```

```{python} 
#| echo: false

# Import required packages.
import zipfile
import os
import pandas as pd
import altair as alt 
import pandas as pd
from datetime import date
import numpy as np
alt.data_transformers.disable_max_rows() 

import json
```

# Background {-}

## Data Download and Exploration (20 points){-} 

1. 

```{python}
# Unzip the datasets
base = (r"C:\Users\hkura\Documents\Uchicago\04 2024 Autumn\Python2\problem-set-6-hirokurachi")
path_zip = os.path.join(
    base, 
    "waze_data.zip"
)

with zipfile.ZipFile(path_zip, "r") as zip_data:
    zip_data.extractall(base)
```
<!--Attribution: Method ".ZipFile()" referring to Perplexity (https://www.perplexity.ai/search/how-to-unzip-a-zip-file-in-the-USeP9tjyQNiVvD50H4LcUw)-->

```{python}
# Load the sample dataset into a DataFrame
path_sample = os.path.join(
    base,
    "waze_data_sample.csv"
)

df_sample = pd.read_csv(path_sample)

# Summarize datatype for each columns
summary_df_sample = pd.DataFrame(df_sample.dtypes).reset_index()
summary_df_sample.columns = ["columns", "datatypes"]

# Ignore the last three columns
summary_df_sample = summary_df_sample.iloc[0:13]

# Fill the datatype column with the altair datatypes
# print(df_sample.head())
alt_types = ["Quantitative", "Nominal", "Quantitative", "Quantitative", "Nominal", "Nominal/Ordinal", "Nominal", "Nominal", "Nominal", "Nominal", "Quantitative", "Quantitative", "Ordinal"]
summary_df_sample["datatypes"] = alt_types

print(summary_df_sample)
```

2. 

```{python}
# Load the total dataset
path_waze = os.path.join(
    base,
    "waze_data.csv"
)
df_waze = pd.read_csv(path_waze)

# Count the number of Nulls and non-Nulls in each column
null_number = [len(df_waze[df_waze[x].isna()]) for x in df_waze.columns]
non_null_number = [len(df_waze[~df_waze[x].isna()]) for x in df_waze.columns]

# Summarize the number of observations fo each columns, with categories of NULL/missing or not
summary_df_waze = dict(zip(["Columns", "NULL", "non_NULL"], [df_waze.columns, null_number, non_null_number]))
summary_df_waze = pd.DataFrame(summary_df_waze)

# Mutate the NULL share
summary_df_waze["NULL_share"] = summary_df_waze["NULL"] / len(df_waze)

print(summary_df_waze)

# Melt the df to specify category (NULL/non-NULL) and whose number for each columns(column names)
summary_df_waze = summary_df_waze.melt(
    id_vars = "Columns",
    var_name = "NULL_or_not",
    value_name = "Number"
)

# Plot a stacked bar of the number of observations for each columns, with categories of NULL/missing or not
summary_df_waze["suborder"] = summary_df_waze["NULL_or_not"].map({"NULL":-1, "non_NULL":1})
chart_null = alt.Chart(summary_df_waze).mark_bar().encode(
    alt.X("Columns:N"),
    alt.Y("Number:Q"),
    alt.Color("NULL_or_not:N", 
        legend = alt.Legend(title = "NULL or not"),
        scale = alt.Scale(
            domain=["NULL", "non_NULL"],
            range=["red", "yellowgreen"]
          )),
    alt.Order("suborder", sort="ascending")
).properties(
    title = "Number of NULLs in each columns",
    height = 300,
    width = 300
)

# Add the numbers of NULL as texts
text_null = alt.Chart(summary_df_waze).mark_text(
    angle = 45
).encode(
    x = "Columns:N",
    text = "Number:N"
).properties(
    height = 300,
    width = 300
).transform_filter(
    "datum.NULL_or_not == 'NULL'"
)

# Integrate the bar chart and the text
chart_null = chart_null + text_null

display(chart_null)
```
<!--Attribution: Method "alt.Order()" and the way or sorting referring to Perplexity (https://www.perplexity.ai/search/how-to-sort-the-piles-of-stack-MVWdMltYSCKbvcpn8X_Kcw)-->


From the result above, the variables which have the NULL is "nThumbsUp", "street" and "subtype", with the highest share of 99.8% for "nThumbsUp".

3. 

```{python}
# Extract Unique types and subtypes
type_unique = df_waze["type"].unique().tolist()
subtype_unique = df_waze["subtype"].unique().tolist()

print(f"type: {type_unique}")
print(f"subtype: {subtype_unique}")

# Define a function to check whether a type include NA subtype
def subtype_isna(type):
    df_waze_bytype = df_waze[df_waze["type"] == type]
    subtype_unique_bytype = df_waze_bytype["subtype"].unique().tolist()
    if np.nan in subtype_unique_bytype:
        return True
    else:
        return False

# Apply the function in list comprehension
type_include_nan = [type for type in type_unique if subtype_isna(type)]
print(f"types which includes subtype nan: {type_include_nan}")
```

Then, we would find out whether the subtypes in each type are informative for their sub-subtypes, by extracting subtype names (including sub-subtype names or not) removing type names from them (This time, we would exclude nan from the checking process).

```{python}
# Summarize the hierarchy of types and subtypes (not including subtype nan)
df_subsubtype = df_waze.groupby(["type", "subtype"]).size().reset_index()

# Remove type name from each subtypes to isolate subtypes from types
# Define function to remove head from base strings
def remove_head(head, base):
    result = base.replace(f"{head}_", "")
    return result
# Isolate subtypes from types by applying the function
df_subsubtype["subtype"] = [remove_head(h, b) for h, b in zip(df_subsubtype["type"], df_subsubtype["subtype"])]

print(df_subsubtype)
```

The result above implies that types "HAZARD" would include sub-subtypes as well as subtypes, because the name of subtypes has some common headers such as "ON_ROAD" or "WEATHER" - which would be the subtype names, while the remainings are the sub-subtype names.

On the other hand, other 3 types doesn't have such characteristics in their subtype names, implying not having sub-subtypes.

Now, the hierarchy of types are like below, excluding nan in each type:

- Accident

    - Major

    - Minor

- Hazard

    - On Road

        - (no sub-subtype)

        - Car stopped

        - Construction

        - Emergency vehicle

        - Ice

        - Object

        - Pot hole

        - Traffic light fault

        - Lane closed

        - Road kill

    - On Shoulder

        - (no sub-subtype)

        - Car stopped

        - Animals

        - Missing sign

    - Weather

        - (no sub-subtype)

        - Flood

        - Fog

        - Heavy snow

        - Hail

- Jam

    - Heavy traffic

    - Moderate traffic

    - Stand still traffic

    - Light traffic

- Road Closed

    - Event

    - Construction

    - Hazard

Finally, we need to keep the NA subtypes, because they count up to 96086 observations with up to about 12.3% share among whole observations, suggesting that ignoring them would affect our findings (especially on total values) significantly. On contrary, by explicitly saying them "Unclassified", we can grasp the whole status of the data, and it is still useful to know how the dataset successfully includes incomplete data.

4. 

a.

```{python}
# Create base df
df_crosswalk = df_waze.copy()

# Create "updated_" columns with temporal values
df_crosswalk["updated_type"] = df_crosswalk["type"]
df_crosswalk["updated_subtype"] = df_crosswalk["subtype"]
df_crosswalk["updated_subsubtype"] = df_crosswalk["subtype"]

# Summarize the hierarchy from the df
df_crosswalk = df_crosswalk.groupby(["type", "subtype", "updated_type", "updated_subtype", "updated_subsubtype"], dropna = False).size().reset_index()
df_crosswalk = df_crosswalk.drop(columns = df_crosswalk.columns[-1], axis = 1)
```

b.

```{python}
# Prepare for "updated_subtype"
# Convert nan's in "updated_subtype" into "Unclassified" to include them in the process
df_crosswalk["updated_subtype"] = df_crosswalk["updated_subtype"].fillna("Unclassified")

# Prepare for "updated_subsubtype"
# Isolate subtypes from types by applying the remove_head() function
df_crosswalk["updated_subsubtype"] = [remove_head(h, b) for h, b in zip(df_crosswalk["type"], df_crosswalk["updated_subtype"])]

# Revise column "updated_subtype"
# Define function to extract subtypes
def extract_subtype(type, subtype):
    if type in  ["ACCIDENT", "JAM", "ROAD_CLOSED"]:
        return subtype.replace(f"{type}_", "")
    elif type == "HAZARD":
        if "ON_ROAD" in subtype:
            return "ON_ROAD"
        elif "ON_SHOULDER" in subtype:
            return "ON_SHOULDER"
        elif "WEATHER" in subtype:
            return "WEATHER"
        elif subtype == "Unclassified":
            return subtype

# Apply the function
df_crosswalk["updated_subtype"] = [extract_subtype(t, s) for t, s in zip(df_crosswalk["type"], df_crosswalk["updated_subtype"])]

# Revise column "updated_subsubtype"
# Isolate sub-subtypes from subtypes by applying the remove_head() function
df_crosswalk["updated_subsubtype"] = [remove_head(h, b) for h, b in zip(df_crosswalk["updated_subtype"], df_crosswalk["updated_subsubtype"])]

# Change them to "Unclassified" if they are the same as the "updated_subtype" (this condition include the case where the subtype is unclassified)
df_crosswalk.loc[df_crosswalk["updated_subsubtype"] == df_crosswalk["updated_subtype"], "updated_subsubtype"] = "unclassified"

# Update the "updated_" columns to a readable format
df_crosswalk.iloc[:, 2:] = df_crosswalk.iloc[:, 2:].map(lambda x: x.replace("_", " ").capitalize())

print(df_crosswalk)
```

c.

```{python}
# Merge the dfs
df_waze_updated = df_waze.merge(
    df_crosswalk,
    how = "inner",
    on = ["type", "subtype"]
)

# Count the rows for Accident - Unclassified
num_Accdnt_Unclssfd = len(df_waze_updated[(df_waze_updated["updated_type"] == "Accident") & (df_waze_updated["updated_subtype"] == "Unclassified")])
print(f"The number of rows for Accident - Unclassified is {num_Accdnt_Unclssfd}.")
```

d.

```{python}

```

1. 
```{python}

```

2. 

```{python}

```

3. 

```{python}

```

4. 

```{python}

```


# App #1: Top Location by Alert Type Dashboard (30 points){-}

1. 

a. 
```{python}

```

b. 
```{python}

```


c. 
```{python}

```

d. 
```{python}

```

3. 
    
a. 

```{python}

```
    

b. 
```{python}
# MODIFY ACCORDINGLY
file_path = "./top_alerts_map/chicago-boundaries.geojson"
#----

with open(file_path) as f:
    chicago_geojson = json.load(f)

geo_data = alt.Data(values=chicago_geojson["features"])

```

4. 

```{python}

```

5. 

a. 

```{python}

```

b. 
```{python}

```

c. 
```{python}

```

d. 
```{python}

```

e. 

# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}

1. 

a. 


    
b. 
```{python}

```

c.

```{python}

```
    

2.

a. 



b. 


c. 


# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}

1. 


a. 

b. 

```{python}

```

2. 

a. 


b. 
    
3. 

a. 
    

b. 


c. 


d.
