---
title: "Problem Set 6 - Waze Shiny Dashboard"
author: "Hiroaki Kurachi"
date: today
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---
1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. 

We use (`*`) to indicate a problem that we think might be time consuming. 

# Steps to submit (10 points on PS6) {-}

1. "This submission is my work alone and complies with the 30538 integrity
policy." Add your initials to indicate your agreement: \*\*HK\*\*
2. "I have uploaded the names of anyone I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  \*\*HK\*\* (2 point)
3. Late coins used this pset: \*\*00\*\* Late coins left after submission: \*\*01\*\*

4. Before starting the problem set, make sure to read and agree to the terms of data usage for the Waze data [here](https://canvas.uchicago.edu/courses/59054/quizzes/130617).

5. Knit your `ps6.qmd` as a pdf document and name it `ps6.pdf`.
6. Push your `ps6.qmd`, `ps6.pdf`, `requirements.txt`, and all created folders (we will create three Shiny apps so you will have at least three additional folders) to your Github repo (5 points). It is fine to use Github Desktop.
7. Submit `ps6.pdf` and also link your Github repo via Gradescope (5 points)
8. Tag your submission in Gradescope. For the Code Style part (10 points) please tag the whole correspondingsection for the code style rubric.

*Notes: see the [Quarto documentation (link)](https://quarto.org/docs/authoring/figures.html) for directions on inserting images into your knitted document.*

*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to "import" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*

```{python}
#| echo: true
#| eval: false

def print_file_contents(file_path):
    """Print contents of a file."""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
            print("```python")
            print(content)
            print("```")
    except FileNotFoundError:
        print("```python")
        print(f"Error: File '{file_path}' not found")
        print("```")
    except Exception as e:
        print("```python") 
        print(f"Error reading file: {e}")
        print("```")

print_file_contents("./top_alerts_map_byhour/app.py") # Change accordingly
```

```{python} 
#| echo: false

# Import required packages.
import zipfile
import os
import pandas as pd
import altair as alt 
alt.renderers.enable("png")
import pandas as pd
from datetime import date
import numpy as np
alt.data_transformers.disable_max_rows() 
import re
import requests

import json
```

# Background {-}

## Data Download and Exploration (20 points){-} 

1. 

```{python}
# Unzip the datasets
base = (r"C:\Users\hkura\Documents\Uchicago\04 2024 Autumn\Python2\problem-set-6-hirokurachi")
path_zip = os.path.join(
    base, 
    "waze_data.zip"
)

with zipfile.ZipFile(path_zip, "r") as zip_data:
    zip_data.extractall(base)
```
<!--Attribution: Method ".ZipFile()" referring to Perplexity (https://www.perplexity.ai/search/how-to-unzip-a-zip-file-in-the-USeP9tjyQNiVvD50H4LcUw)-->

```{python}
# Load the sample dataset into a DataFrame
path_sample = os.path.join(
    base,
    "waze_data_sample.csv"
)

df_sample = pd.read_csv(path_sample)

# Summarize datatype for each columns
summary_df_sample = pd.DataFrame(df_sample.dtypes).reset_index()
summary_df_sample.columns = ["columns", "datatypes"]

# Ignore the last three columns
summary_df_sample = summary_df_sample.iloc[0:13]

# Fill the datatype column with the altair datatypes
alt_types = ["Quantitative", "Nominal", "Ordinal", "Quantitative", "Nominal", "Nominal", "Nominal", "Nominal", "Nominal", "Nominal", "Ordinal", "Quantitative", "Ordinal"]
summary_df_sample["datatypes"] = alt_types

print(summary_df_sample)
```

2. 

```{python}
# Load the total dataset
path_waze = os.path.join(
    base,
    "waze_data.csv"
)
df_waze = pd.read_csv(path_waze)

# Count the number of Nulls and non-Nulls in each column
null_number = [len(df_waze[df_waze[x].isna()]) for x in df_waze.columns]
non_null_number = [len(df_waze[~df_waze[x].isna()]) for x in df_waze.columns]

# Summarize the number of observations fo each columns, with categories of NULL/missing or not
summary_df_waze = dict(
    zip(
        ["Columns", "NULL", "non_NULL"], 
        [df_waze.columns, null_number, non_null_number]
    )
)
summary_df_waze = pd.DataFrame(summary_df_waze)

# Mutate the NULL share
summary_df_waze["NULL_share"] = summary_df_waze["NULL"] / len(df_waze)

print(summary_df_waze)

# Melt the df to specify category (NULL/non-NULL) and whose number for each columns(column names)
summary_df_waze = summary_df_waze.melt(
    id_vars = "Columns",
    var_name = "NULL_or_not",
    value_name = "Number"
)

# Plot a stacked bar of the number of observations for each columns, with categories of NULL/missing or not
summary_df_waze["suborder"] = summary_df_waze["NULL_or_not"].map(
    {"NULL":-1, "non_NULL":1}
)
chart_null = alt.Chart(summary_df_waze).mark_bar().encode(
    alt.X("Columns:N"),
    alt.Y("Number:Q"),
    alt.Color("NULL_or_not:N", 
        legend = alt.Legend(title = "NULL or not"),
        scale = alt.Scale(
            domain=["NULL", "non_NULL"],
            range=["red", "yellowgreen"]
          )),
    alt.Order("suborder", sort="ascending")
).properties(
    title = "Number of NULLs in each columns",
    height = 300,
    width = 300
)

# Add the numbers of NULL as texts
text_null = alt.Chart(summary_df_waze).mark_text(
    angle = 45
).encode(
    x = "Columns:N",
    text = "Number:N"
).properties(
    height = 300,
    width = 300
).transform_filter(
    "datum.NULL_or_not == 'NULL'"
)

# Integrate the bar chart and the text
chart_null = chart_null + text_null

chart_null.show()
```
<!--Attribution: Method "alt.Order()" and the way or sorting referring to Perplexity (https://www.perplexity.ai/search/how-to-sort-the-piles-of-stack-MVWdMltYSCKbvcpn8X_Kcw)-->


From the result above, the variables which have the NULL is "nThumbsUp", "street" and "subtype", with the highest share of 99.8% for "nThumbsUp".

3. 

a.
```{python}
# Extract Unique types and subtypes
type_unique = df_waze["type"].unique().tolist()
subtype_unique = df_waze["subtype"].unique().tolist()

print(f"type: {type_unique}")
print(f"subtype: {subtype_unique}")

# Define a function to check whether a type include NA subtype
def subtype_isna(type):
    df_waze_bytype = df_waze[df_waze["type"] == type]
    subtype_unique_bytype = df_waze_bytype["subtype"].unique().tolist()
    if np.nan in subtype_unique_bytype:
        return True
    else:
        return False

# Apply the function in list comprehension
type_include_nan = [type for type in type_unique if subtype_isna(type)]
print(f"types which includes subtype nan: {type_include_nan}")
```

Then, we would find out whether the subtypes in each type are informative for their sub-subtypes, by extracting subtype names (including sub-subtype names or not) removing type names from them (This time, we would exclude nan from the checking process).

```{python}
# Summarize the hierarchy of types and subtypes (not including nan in subtype)
df_hierarchy = df_waze.groupby(["type", "subtype"]).size().reset_index()

# Remove type name from each subtypes to isolate subtypes from types
# Define function to remove head from base strings
def remove_head(head, base):
    result = base.replace(f"{head}_", "")
    return result
# Isolate subtypes from types by applying the function
df_hierarchy["subtype"] = [remove_head(h, b) for h, b in zip(
    df_hierarchy["type"], df_hierarchy["subtype"]
)]

print(df_hierarchy)
```

The result above implies that types "HAZARD" would include sub-subtypes as well as subtypes, because the name of subtypes has some common headers such as "ON_ROAD" or "WEATHER" - which would be the subtype names, while the remainings are the sub-subtype names.

On the other hand, other 3 types doesn't have such characteristics in their subtype names, implying not having sub-subtypes.

b. 

Now, the hierarchy of types are like below, excluding nan in each type:

- Accident

    - Major

    - Minor

- Hazard

    - On Road

        - (no sub-subtype)

        - Car stopped

        - Construction

        - Emergency vehicle

        - Ice

        - Object

        - Pot hole

        - Traffic light fault

        - Lane closed

        - Road kill

    - On Shoulder

        - (no sub-subtype)

        - Car stopped

        - Animals

        - Missing sign

    - Weather

        - (no sub-subtype)

        - Flood

        - Fog

        - Heavy snow

        - Hail

- Jam

    - Heavy traffic

    - Moderate traffic

    - Stand still traffic

    - Light traffic

- Road Closed

    - Event

    - Construction

    - Hazard

c.

Finally, we need to keep the NA subtypes, because they count up to 96086 observations with up to about 12.3% share among whole observations, suggesting that completely dropping these rows would lose significant amount of samples from the population, with risks of affecting our statistical analysis severely and significantly. On contrary, by explicitly saying them "Unclassified", we can recognize them without confusion. And we can grasp the whole quality of the dataset itself, as it is still useful to know how the dataset successfully includes incomplete data if there are no problem in dealing with the data.

4. 

a.

```{python}
# Create base df for crosswalk
df_crosswalk = df_waze.copy()

# Create "updated_" columns with temporal values
df_crosswalk["updated_type"] = df_crosswalk["type"]
df_crosswalk["updated_subtype"] = df_crosswalk["subtype"]
df_crosswalk["updated_subsubtype"] = df_crosswalk["subtype"]

# Summarize the hierarchy from the df
df_crosswalk = df_crosswalk.groupby(
    ["type", "subtype", "updated_type", "updated_subtype", "updated_subsubtype"], 
    dropna = False
).size().reset_index()
df_crosswalk = df_crosswalk.drop(
    columns = df_crosswalk.columns[-1], 
    axis = 1
)
```

b.

```{python}
# Prepare for "updated_subtype"
# Convert nan's in "updated_subtype" into "Unclassified" to include them in the process
df_crosswalk["updated_subtype"] = df_crosswalk["updated_subtype"].fillna("Unclassified")

# Prepare for "updated_subsubtype"
# Isolate subtypes from types by applying the remove_head() function
df_crosswalk["updated_subsubtype"] = [remove_head(h, b) for h, b in zip(df_crosswalk["type"], df_crosswalk["updated_subtype"])]

# Revise column "updated_subtype"
# Define function to extract subtypes
def extract_subtype(type, subtype):
    if type in  ["ACCIDENT", "JAM", "ROAD_CLOSED"]:
        return subtype.replace(f"{type}_", "")
    elif type == "HAZARD":
        if "ON_ROAD" in subtype:
            return "ON_ROAD"
        elif "ON_SHOULDER" in subtype:
            return "ON_SHOULDER"
        elif "WEATHER" in subtype:
            return "WEATHER"
        elif subtype == "Unclassified":
            return subtype

# Apply the function
df_crosswalk["updated_subtype"] = [extract_subtype(t, s) for t, s in zip(df_crosswalk["type"], df_crosswalk["updated_subtype"])]

# Revise column "updated_subsubtype"
# Isolate sub-subtypes from subtypes by applying the remove_head() function
df_crosswalk["updated_subsubtype"] = [remove_head(h, b) for h, b in zip(df_crosswalk["updated_subtype"], df_crosswalk["updated_subsubtype"])]

# Change them to "Unclassified" if they are the same as the "updated_subtype" (this condition include the case where the subtype is unclassified)
df_crosswalk.loc[df_crosswalk["updated_subsubtype"] == df_crosswalk["updated_subtype"], "updated_subsubtype"] = "unclassified"

# Update the "updated_" columns to a readable format
df_crosswalk.iloc[:, 2:] = df_crosswalk.iloc[:, 2:].map(lambda x: x.replace("_", " ").capitalize())
```

c.

```{python}
# Merge the dfs
df_waze_updated = df_waze.merge(
    df_crosswalk,
    how = "inner",
    on = ["type", "subtype"]
)

# Count the rows for Accident - Unclassified
num_Accdnt_Unclssfd = len(df_waze_updated[
    (df_waze_updated["updated_type"] == "Accident") & (df_waze_updated["updated_subtype"] == "Unclassified")
])
print(f"The number of rows for Accident - Unclassified is {num_Accdnt_Unclssfd}.")
```

d.

```{python}

```

# App #1: Top Location by Alert Type Dashboard (30 points){-}

1. 

a. 
```{python}
# Split the strings column "geo" at every white-space character
base_list_geo = df_waze_updated["geo"].map(lambda x: re.split(r"\s", x))

# Convert the series of lists into df
df_geo = pd.DataFrame(base_list_geo.tolist(), columns = ["longitude", "latitude"])

# Remove redundant strings from the columns
df_geo["longitude"] = df_geo["longitude"].map(lambda x: float(x.replace("POINT(", "")))
df_geo["latitude"] = df_geo["latitude"].map(lambda x: float(x.replace(")", "")))

# Join the df into the base df
df_waze_updated = df_waze_updated.join(df_geo)
```
<!--Attribution: Method "re.split()" referring to W3Schools (https://www.w3schools.com/python/python_regex.asp#split)-->

b. 
```{python}
# Bin the longitude and latitude variables
df_waze_updated[["longitude", "latitude"]] = df_waze_updated[["longitude", "latitude"]].map(lambda x: round(x, 2))

# Count each group by latitude and longitude
summary_df_geo = df_waze_updated.groupby(
    ["longitude", "latitude"]
).size().reset_index().rename(
    columns = {0 : "count"}
).sort_values(
    "count", 
    ascending = False
)

print(summary_df_geo.head(1))
```

Thus, the set of binned latitude-longitude combination above has the greatest number of observations in the overall dataset.

c. 
```{python}
# Summarize the df by type, subtype, latitude and longitude
df_top_alerts_maps = df_waze_updated.groupby(
    ["updated_type", "updated_subtype", "longitude", "latitude"]
).size().reset_index().rename(
    columns = {0 : "count"}
)

# Group and filter top 10 from the sorted df by type and subtype
df_top_alerts_maps = df_top_alerts_maps.sort_values(
    "count", 
    ascending = False
).groupby(
    ["updated_type", "updated_subtype"]
).head(10).reset_index(drop = True)

df_top_alerts_maps.to_csv(r"top_alerts_map\top_alerts_map.csv")
```

LEVEL OF AGGREGATION
And the DataFrame has 155 rows.

2. 
```{python}
# Plot the alerts by latitude and longitude for "Jam-Heavy Traffic" cases
chart_alert = alt.Chart(df_top_alerts_maps).mark_point(
    color = "firebrick",
    filled = True
).encode(
    alt.X(
        "longitude:Q", 
        scale = alt.Scale(
            domain = [-87.80, -87.60]
        )
    ),
    alt.Y(
        "latitude:Q", 
        scale = alt.Scale(
            domain = [41.60, 42.00]
        )
    ),    
    alt.Size(
        "count:Q", 
        scale = alt.Scale(
            domain = [2400, 4500]
        ), 
        legend = alt.Legend(
            title = "Number of Alerts"
        )
    )
).properties(
    title = "Top 10 Areas of 'Jam - Heavy Traffic' Alerts Number",
    height = 300,
    width = 300
).transform_filter(
    "datum.updated_type == 'Jam' & datum.updated_subtype == 'Heavy traffic'"
)

display(chart_alert)
```

3. 
    
a. 

```{python}
# Download and save from the url
url_geojson = "https://data.cityofchicago.org/api/geospatial/bbvz-uum9?method=export&format=GeoJSON"

response = requests.get(url_geojson)
data = response.json()  

# Save as a geojson file
path_json = os.path.join(
    base, 
    r"top_alerts_map\chicago-boundaries.geojson"
)

with open(path_json, "w") as f:
    json.dump(data, f)
```
<!--Attribution: method ".json" referring to Real Python (https://realpython.com/python-requests/)-->

<!--Attribution: json package referring to Perplexity (https://www.perplexity.ai/search/methods-in-json-package-for-sa-KX2RBtBjTLasvWgV9UzWgw)-->    

b. 
```{python}
# Load the geojson file
with open(path_json) as f:
    chicago_geojson = json.load(f)

geo_data = alt.Data(values = chicago_geojson["features"])
```

4. 

```{python}
# Create subset
df_top_alerts_maps_chosen = df_top_alerts_maps[(
    df_top_alerts_maps["updated_type"] == "Jam"
) & (
    df_top_alerts_maps["updated_subtype"] == "Heavy traffic"
)]

# Set appropriate domain for chosen type and subtype
domain_1 = [
    df_top_alerts_maps_chosen["count"].min(), 
    df_top_alerts_maps_chosen["count"].max()
]

# Redefine the scatterplot
chart_alert = alt.Chart(df_top_alerts_maps_chosen).mark_point(
    color = "firebrick",
    filled = True
).encode( 
    longitude = "longitude:Q",
    latitude = "latitude:Q",
    size = alt.Size(
        "count:Q", 
        scale = alt.Scale(
            domain = domain_1
        ), 
        legend = alt.Legend(
            title = "Number of Alerts"
        )
    )
).properties(
    title = "Top 10 Areas of 'Jam - Heavy Traffic' Alerts Number",
    height = 300,
    width = 300
).transform_filter(
    "datum.updated_type == 'Jam' & datum.updated_subtype == 'Heavy traffic'"
)

# Plot map
chart_map = alt.Chart(geo_data).mark_geoshape(
    fill = "lightgray",
    stroke = "white"
).project(
    type = "equirectangular"
)


# Overlay the plots
chart_alert_map = chart_map + chart_alert

chart_alert_map.show()
```

5. 

a. 

![](app1_5a.png){width = 200, height = 200}

There are 16 total combinations.

b. 

![](app1_5b.png){width = 200, height = 200}

c. 

![](app1_5c.png){width = 200, height = 200}

WHERE IS THE MOST???

d. 

FORMULATE QUESTION

![](app1_5d.png){width = 200, height = 200}

ANSWER

e. 

ADD WHICH COLUMN?

# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}

1. 

a. 

Collapsing the dataset by the column *ts* would be not a good idea. We of course need to subset the dataset based on the information in *ts*, but we need to create a new column, extracting an hour of the day from each value in the column. There, we should remove redundant information, such as year-month-date(irrelevant in our analysis), or minute and seconds (too detailed for input in UI, not requested), or "UTC" (All data in the column are UTC).
    
b.  
```{python}
# Create "hour" column
df_waze_updated["hour"] = pd.to_datetime(df_waze_updated["ts"].replace("UTC", "")).dt.strftime("%H:00")

# Summarize the df by type, subtype, latitude, longitude and hour
df_top_alerts_maps_byhour = df_waze_updated.groupby(
    ["updated_type", "updated_subtype", "longitude", "latitude", "hour"]
).size().reset_index().rename(
    columns = {0 : "count"}
)

# Group and filter top 10 from the sorted df by hour
df_top_alerts_maps_byhour = df_top_alerts_maps_byhour.sort_values(
    "count", 
    ascending = False
).groupby(
    ["updated_type", "updated_subtype", "hour"]
).head(10).reset_index(drop = True)

df_top_alerts_maps_byhour.to_csv(r"top_alerts_map_byhour\top_alerts_map_byhour.csv")
```

LEVEL OF AGGREGATION
It includes 240 rows.

c.

```{python}
# Create subset
df_top_alerts_maps_byhour_chosen = df_top_alerts_maps_byhour[(
    df_top_alerts_maps_byhour["updated_type"] == "Jam"
) & (
    df_top_alerts_maps_byhour["updated_subtype"] == "Heavy traffic"
) & (
    df_top_alerts_maps_byhour["hour"].isin(["10:00", "16:00", "23:00"])
)]

# Set appropriate domain for chosen type and subtype
domain_2 = [
    df_top_alerts_maps_byhour_chosen["count"].min(), 
    df_top_alerts_maps_byhour_chosen["count"].max()
]

# Plot the alerts by longitude and latitude for "Jam-Heavy Traffic" cases, for 3 timings
chart_alert = alt.Chart().mark_point(
    color = "firebrick",
    filled = True
).encode(
    longitude = "longitude:Q",
    latitude = "latitude:Q",
    size = alt.Size(
        "count:Q",
        scale = alt.Scale(
            domain = domain_2
        ),  
        legend = alt.Legend(
            title = "Number of Alerts"
        ))
).properties(
    height = 200,
    width = 200
)

chart_alert_map_byhour = alt.layer(chart_map, chart_alert).facet(
    data = df_top_alerts_maps_byhour_chosen,
    column = "hour:N"
).properties(
    title = "Top 10 Areas of 'Jam - Heavy Traffic' Alerts Number"
)

chart_alert_map_byhour.show()
```
    

2.

a. 

![](app2_2a.png){width = 200, height = 200}

b. 

![](app2_2b.png){width = 200, height = 200}

c. 

![](app2_2c-morning.png){width = 200, height = 200}

![](app2_2c-night.png){width = 200, height = 200}

# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}

1. 


a. 

COLLAPSING BY RANGE IS GOOD OR NOT??
We cannot define the range of numbers for format "HH:MM" with ".range()" method. Currently, the column "hour" contains strings, thus we cannot get range from chosen hours based on the column. And even if we 

b. 

```{python}
# Create base column for calculating range of hours
df_top_alerts_maps_byhour["hour_dt"] = pd.to_datetime(df_top_alerts_maps_byhour["hour"])
df_top_alerts_maps_byhour["hour_dt"] = [x.time() for x in df_top_alerts_maps_byhour["hour_dt"]]

# Create subset
df_top_alerts_maps_byhour_range = df_top_alerts_maps_byhour[(
    df_top_alerts_maps_byhour["updated_type"] == "Jam"
) & (
    df_top_alerts_maps_byhour["updated_subtype"] == "Heavy traffic"
) & (
    df_top_alerts_maps_byhour["hour_dt"].between(time(6, 0), time(9, 0))
)]

# Set appropriate domain for chosen type and subtype
domain_3 = [
    df_top_alerts_maps_byhour_range["count"].min(), 
    df_top_alerts_maps_byhour_range["count"].max()
]

# Plot the alerts by longitude and latitude for "Jam-Heavy Traffic" cases, for 3 timings
chart_alert = alt.Chart().mark_point(
    color = "firebrick",
    filled = True
).encode(
    longitude = "longitude:Q",
    latitude = "latitude:Q",
    size = alt.Size(
        "count:Q",
        scale = alt.Scale(
            domain = domain_3
        ),  
        legend = alt.Legend(
            title = "Number of Alerts"
        ))
).properties(
    height = 200,
    width = 200
)

chart_alert_map_byhour = alt.layer(chart_map, chart_alert).facet(
    data = df_top_alerts_maps_byhour_range,
    column = "hour:N"
).properties(
    title = "Top 10 Areas of 'Jam - Heavy Traffic' Alerts Number"
)

chart_alert_map_byhour.show()
```
<!--Attribution: Method ".time()" referring to Perplexity (https://www.perplexity.ai/search/how-to-make-date-time-object-w-bkYUZPR5R.6jqOOnOhDIzw)-->
<!--Attribution: Method ".between()" referring to Perplexity (https://www.perplexity.ai/search/df-top-alerts-maps-byhour-hour-vtZei4OvTCCm.fD0riMcVg)-->

2. 

a. 


b. 
    
3. 

a. 
    

b. 


c. 


d.
