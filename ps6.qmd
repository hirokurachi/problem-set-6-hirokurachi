---
title: "Problem Set 6 - Waze Shiny Dashboard"
author: "Hiroaki Kurachi"
date: today
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---
1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. 

We use (`*`) to indicate a problem that we think might be time consuming. 

# Steps to submit (10 points on PS6) {-}

1. "This submission is my work alone and complies with the 30538 integrity
policy." Add your initials to indicate your agreement: \*\*HK\*\*
2. "I have uploaded the names of anyone I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  \*\*HK\*\* (2 point)
3. Late coins used this pset: \*\*00\*\* Late coins left after submission: \*\*01\*\*

4. Before starting the problem set, make sure to read and agree to the terms of data usage for the Waze data [here](https://canvas.uchicago.edu/courses/59054/quizzes/130617).

5. Knit your `ps6.qmd` as a pdf document and name it `ps6.pdf`.
6. Push your `ps6.qmd`, `ps6.pdf`, `requirements.txt`, and all created folders (we will create three Shiny apps so you will have at least three additional folders) to your Github repo (5 points). It is fine to use Github Desktop.
7. Submit `ps6.pdf` and also link your Github repo via Gradescope (5 points)
8. Tag your submission in Gradescope. For the Code Style part (10 points) please tag the whole correspondingsection for the code style rubric.

*Notes: see the [Quarto documentation (link)](https://quarto.org/docs/authoring/figures.html) for directions on inserting images into your knitted document.*

*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to "import" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*

```{python}
# | echo: true

def print_file_contents(file_path):
    """Print contents of a file."""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
            print("```python")
            print(content)
            print("```")
    except FileNotFoundError:
        print("```python")
        print(f"Error: File '{file_path}' not found")
        print("```")
    except Exception as e:
        print("```python")
        print(f"Error reading file: {e}")
        print("```")
```

```{python} 
# Import required packages.
import zipfile
import os
import pandas as pd
import altair as alt
import pandas as pd
from datetime import date, time
import numpy as np
import re
import requests
import json
alt.renderers.enable("png")
alt.data_transformers.disable_max_rows() 
```

# Background {-}

## Data Download and Exploration (20 points){-} 

1. 

```{python}
# Unzip the datasets
base = (r"C:\Users\hkura\Documents\Uchicago\04 2024 Autumn\Python2\problem-set-6-hirokurachi")
path_zip = os.path.join(
    base,
    "waze_data.zip"
)

with zipfile.ZipFile(path_zip, "r") as zip_data:
    zip_data.extractall(base)
```
<!--Attribution: Method ".ZipFile()" referring to Perplexity (https://www.perplexity.ai/search/how-to-unzip-a-zip-file-in-the-USeP9tjyQNiVvD50H4LcUw)-->

```{python}
# Load the sample dataset into a DataFrame
path_sample = os.path.join(
    base,
    "waze_data_sample.csv"
)

df_sample = pd.read_csv(path_sample)

# Summarize datatype for each columns
df_sample_dtypes = pd.DataFrame(df_sample.dtypes).reset_index()
df_sample_dtypes.columns = ["columns", "datatypes"]

# Fill the datatype column with the altair datatypes
alt_datatypes = ["Quantitative", "Nominal", "Ordinal", "Quantitative", "Nominal", "Nominal", "Nominal",
             "Nominal", "Nominal", "Nominal", "Ordinal", "Quantitative", "Ordinal", np.nan, np.nan, np.nan]
df_sample_dtypes["datatypes"] = alt_datatypes

print(df_sample_dtypes)
```

2. 

```{python}
# Load the total dataset
path_waze = os.path.join(
    base,
    "waze_data.csv"
)
df_waze = pd.read_csv(path_waze)

# Count the number of Nulls and non-Nulls in each column
null_number = [len(df_waze[df_waze[x].isna()]) for x in df_waze.columns]
non_null_number = [len(df_waze[~df_waze[x].isna()]) for x in df_waze.columns]

# Summarize the number of observations fo each columns, with categories of NULL/missing or not
df_nullshare_waze = pd.DataFrame(
    dict(zip(
        ["Columns", "NULL", "non_NULL"], 
        [df_waze.columns, null_number, non_null_number])
    )
)

# Mutate the NULL share
df_nullshare_waze["NULL_share"] = df_nullshare_waze["NULL"] / len(df_waze)

print(df_nullshare_waze)

# Melt the df to specify category (NULL/non-NULL) and whose number for each columns(column names)
df_null_or_not_waze = df_nullshare_waze.melt(
    id_vars="Columns",
    var_name="NULL_or_not",
    value_name="Number"
)

# Plot a stacked bar of the number of observations for each columns, with categories of NULL/missing or not
df_null_or_not_waze["bool_Null_or_not"] = df_null_or_not_waze["NULL_or_not"].map(
    {"NULL": -1, "non_NULL": 1}
)
chart_null = alt.Chart(df_null_or_not_waze).mark_bar().encode(
    alt.X("Columns:N"),
    alt.Y("Number:Q"),
    alt.Color("NULL_or_not:N",
              legend=alt.Legend(title="NULL or not"),
              scale=alt.Scale(
                  domain=["NULL", "non_NULL"],
                  range=["red", "yellowgreen"]
              )),
    alt.Order("bool_Null_or_not", sort="ascending")
).properties(
    title="Number of NULLs in each columns",
    height=300,
    width=300
)

# Add the numbers of NULL as texts
text_null = alt.Chart(df_null_or_not_waze).mark_text(
    angle=45
).encode(
    x="Columns:N",
    text="Number:N"
).properties(
    height=300,
    width=300
).transform_filter(
    "datum.NULL_or_not == 'NULL'"
)

# Integrate the bar chart and the text
chart_null = chart_null + text_null

chart_null.show()
```
<!--Attribution: Method "alt.Order()" and the way or sorting referring to Perplexity (https://www.perplexity.ai/search/how-to-sort-the-piles-of-stack-MVWdMltYSCKbvcpn8X_Kcw)-->


From the result above, the variables which have the NULL is "nThumbsUp", "street" and "subtype", with the highest share of 99.8% for "nThumbsUp".

3. 

a.
```{python}
# Extract Unique types and subtypes
type_unique = df_waze["type"].unique().tolist()
subtype_unique = df_waze["subtype"].unique().tolist()

print(f"type: {type_unique}")
print(f"subtype: {subtype_unique}")
```

```{python}
# Find out types which have a subtype that is NA


def subtype_isna(type):
    """Check whether a type include NA subtype"""
    df_waze_bytype = df_waze[df_waze["type"] == type]
    subtype_unique_bytype = df_waze_bytype["subtype"].unique().tolist()
    if np.nan in subtype_unique_bytype:
        return True
    else:
        return False


type_include_nan = [type for type in type_unique if subtype_isna(type)]
print(f"types which includes subtype nan: {type_include_nan}")
```

All of four types have a subtype that is NA.

Then, we would find out whether the subtypes in each type are informative for their sub-subtypes, by extracting subtype names (including sub-subtype names or not) removing type names from them (This time, we would exclude nan from the checking process).

```{python}
# Summarize the hierarchy of types and subtypes (not including nan in subtype)
df_hierarchy = df_waze.groupby(
    ["type", "subtype"]
).size().reset_index().rename(
    columns={0: "count"}
)

# Define general function to remove parent name from children name in the categorical hierarchy



def remove_head(head, base):
    """Remove head strings from base strings"""
    result = base.replace(f"{head}_", "")
    return result


# Remove type name from subtype names
df_hierarchy["subtype"] = [remove_head(h, b) for h, b in zip(
    df_hierarchy["type"], df_hierarchy["subtype"]
)]

print(df_hierarchy)
```

The result above implies that types "HAZARD" would include sub-subtypes as well as subtypes, because the name of subtypes has some common head strings such as "ON_ROAD" or "WEATHER" - which would be the isolated subtype names, while the remainings are the isolated sub-subtype names.

On the other hand, other 3 types doesn't have such characteristics in their subtype names, implying not having sub-subtypes.

b. 

Now, the hierarchy of types are like below, excluding subtype of nan in each type:

- Accident

    - Major

    - Minor

- Hazard

    - On Road

        - (no sub-subtype)

        - Car stopped

        - Construction

        - Emergency vehicle

        - Ice

        - Lane closed

        - Object

        - Pot hole

        - Road kill

        - Traffic light fault

    - On Shoulder

        - (no sub-subtype)

        - Animals

        - Car stopped

        - Missing sign

    - Weather

        - (no sub-subtype)

        - Flood

        - Fog

        - Hail

        - Heavy snow

- Jam

    - Heavy traffic

    - Light traffic

    - Moderate traffic

    - Stand still traffic

- Road Closed

    - Construction

    - Event

    - Hazard

c.

We need to keep the NA subtypes, because they count up to 96086 observations with up to about 12.3% share among whole observations, suggesting that completely dropping these rows would lose significant amount of samples from the population, with risks of affecting our statistical analysis severely and significantly. On contrary, by explicitly saying them "Unclassified", we can recognize them without confusion. And it is still useful to grasp how the dataset successfully includes incomplete data as far as there are no problem in dealing with the data.

4. 

a.

```{python}
# Create base df for crosswalk
df_crosswalk = df_waze.copy()

# Summarize the df so that it has rows for each set of type and subtype
df_crosswalk = df_crosswalk.groupby(
    ["type", "subtype"],
    dropna=False
).size().reset_index()

df_crosswalk = df_crosswalk.rename(
    columns={0: "count"}
).drop("count", axis=1)

# Create "updated_" columns with temporal base values
df_crosswalk["updated_type"] = df_crosswalk["type"]
df_crosswalk["updated_subtype"] = df_crosswalk["subtype"]
df_crosswalk["updated_subsubtype"] = df_crosswalk["subtype"]

print(df_crosswalk.head(3))
```

b.

```{python}
# Convert nan in "updated_subtype" and "updated_subsubtype" into "Unclassified" to make them recognized as strings in following process
df_crosswalk["updated_subtype"] = df_crosswalk["updated_subtype"].fillna(
    "Unclassified")
df_crosswalk["updated_subsubtype"] = df_crosswalk["updated_subtype"].fillna(
    "Unclassified")

# Remove type name from "updated_subsubtype" to get subtype + sub-subtype
df_crosswalk["updated_subsubtype"] = [remove_head(h, b) for h, b in zip(
    df_crosswalk["type"], df_crosswalk["updated_subsubtype"])]

# Isolate subtype names in "updated_subtype"


def extract_subtype(type, subtype):
    """Extract isolated subtype name"""
    # For types without sub-subtypes, just remove type name
    if type in ["ACCIDENT", "JAM", "ROAD_CLOSED"]:
        return subtype.replace(f"{type}_", "")
    # For type "Hazard", check which isolated subtype name is included
    elif type == "HAZARD":
        if "ON_ROAD" in subtype:
            return "ON_ROAD"
        elif "ON_SHOULDER" in subtype:
            return "ON_SHOULDER"
        elif "WEATHER" in subtype:
            return "WEATHER"
        elif subtype == "Unclassified":
            return subtype


df_crosswalk["updated_subtype"] = [extract_subtype(t, s) for t, s in zip(
    df_crosswalk["type"], df_crosswalk["updated_subtype"])]

# Remove isolated subtype name from "updated_subsubtype" to get isolated sub-subtype
df_crosswalk["updated_subsubtype"] = [remove_head(h, b) for h, b in zip(
    df_crosswalk["updated_subtype"], df_crosswalk["updated_subsubtype"])]

# Change sub-subtypes to "Unclassified" if they are the same as the "updated_subtype" (this condition include the case where the subtype is "unclassified")
df_crosswalk.loc[df_crosswalk["updated_subsubtype"] ==
                 df_crosswalk["updated_subtype"], "updated_subsubtype"] = "unclassified"

# Update the "updated_" columns to a readable format
df_crosswalk.iloc[:, 2:] = df_crosswalk.iloc[:, 2:].map(
    lambda x: x.replace("_", " ").capitalize())

print(df_crosswalk)
```

c.

```{python}
# Merge the dfs
df_waze_updated = df_waze.merge(
    df_crosswalk,
    how="inner",
    on=["type", "subtype"]
)

# Count the rows for Accident - Unclassified
num_Accdnt_Unclssfd = len(df_waze_updated[
    (df_waze_updated["updated_type"] == "Accident") & (
        df_waze_updated["updated_subtype"] == "Unclassified")
])
print(
    f"The number of rows for Accident - Unclassified is {num_Accdnt_Unclssfd}."
)
```

d.

```{python}

```

# App #1: Top Location by Alert Type Dashboard (30 points){-}

1. 

a. 
```{python}
# Split the strings column "geo" at every white-space character
series_geo_lists = df_waze_updated["geo"].map(lambda x: re.split(r"\s", x))

# Convert the series of lists into df
df_coordinate = pd.DataFrame(series_geo_lists.tolist(), columns=[
                      "longitude", "latitude"])

# Remove redundant strings from the columns
df_coordinate["longitude"] = df_coordinate["longitude"].map(
    lambda x: float(x.replace("POINT(", "")))
df_coordinate["latitude"] = df_coordinate["latitude"].map(
    lambda x: float(x.replace(")", "")))

# Join the df into the base df
df_waze_updated = df_waze_updated.join(df_coordinate)

print(df_waze_updated[["longitude", "latitude"]].head(3))
```
<!--Attribution: Method "re.split()" referring to W3Schools (https://www.w3schools.com/python/python_regex.asp#split)-->

b. 
```{python}
# Bin the longitude and latitude variables
df_waze_updated[["longitude", "latitude"]] = df_waze_updated[[
    "longitude", "latitude"]].map(lambda x: round(x, 2))

# Count each group by latitude and longitude
df_coordinate_count = df_waze_updated.groupby(
    ["longitude", "latitude"]
).size().reset_index().rename(
    columns={0: "count"}
).sort_values(
    "count",
    ascending=False
)

print(df_coordinate_count.head(1))
```

The set of binned latitude-longitude combination above has the greatest number of observations in the overall dataset.

c. 
```{python}
# Summarize the df by type, subtype, latitude and longitude
df_top_alerts_map = df_waze_updated.groupby(
    ["updated_type", "updated_subtype", "longitude", "latitude"]
).size().reset_index().rename(
    columns={0: "count"}
)

# Group and filter top 10 from the sorted df by type and subtype
df_top_alerts_map = df_top_alerts_map.sort_values(
    "count",
    ascending=False
).groupby(
    ["updated_type", "updated_subtype"]
).head(10).reset_index(drop=True)

df_top_alerts_map.to_csv(r"top_alerts_map\top_alerts_map.csv")
```

The level of aggregation is (sets of) type and subtype.

```{python}
print(f"And the DataFrame has {len(df_top_alerts_map)} rows.")
```


2. 
```{python}
# Plot the alerts by latitude and longitude for "Jam-Heavy Traffic" cases
chart_alerts = alt.Chart(df_top_alerts_map).mark_point(
    color="firebrick",
    filled=True
).encode(
    alt.X(
        "longitude:Q",
        scale=alt.Scale(
            domain=[-87.80, -87.60]
        )
    ),
    alt.Y(
        "latitude:Q",
        scale=alt.Scale(
            domain=[41.60, 42.00]
        )
    ),
    alt.Size(
        "count:Q",
        scale=alt.Scale(
            domain=[2400, 4500]
        ),
        legend=alt.Legend(
            title="Number of Alerts"
        )
    )
).properties(
    title="Top 10 Areas of 'Jam - Heavy Traffic' Alerts Number",
    height=300,
    width=300
).transform_filter(
    "datum.updated_type == 'Jam' & datum.updated_subtype == 'Heavy traffic'"
)

chart_alerts.show()
```

3. 
    
a. 

```{python}
# Download and save from the url
url_geojson = "https://data.cityofchicago.org/api/geospatial/bbvz-uum9?method=export&format=GeoJSON"

response = requests.get(url_geojson)
data = response.json()

# Save as a geojson file
path_json = os.path.join(
    base,
    r"top_alerts_map\chicago-boundaries.geojson"
)

with open(path_json, "w") as f:
    json.dump(data, f)
```
<!--Attribution: method ".json" referring to Real Python (https://realpython.com/python-requests/)-->
<!--Attribution: json package referring to Perplexity (https://www.perplexity.ai/search/methods-in-json-package-for-sa-KX2RBtBjTLasvWgV9UzWgw)-->    

b. 
```{python}
# Load the geojson file
with open(path_json) as f:
    chicago_geojson = json.load(f)

geo_data = alt.Data(values=chicago_geojson["features"])
```

4. 

```{python}
# Create subset
df_top_alerts_map_chosen = df_top_alerts_map[(
    df_top_alerts_map["updated_type"] == "Jam"
) & (
    df_top_alerts_map["updated_subtype"] == "Heavy traffic"
)]

# Set appropriate domain for chosen type and subtype
domain_1 = [
    df_top_alerts_map_chosen["count"].min(),
    df_top_alerts_map_chosen["count"].max()
]

# Redefine the scatterplot
chart_alerts = alt.Chart(df_top_alerts_map_chosen).mark_point(
    color="firebrick",
    filled=True
).encode(
    longitude="longitude:Q",
    latitude="latitude:Q",
    size=alt.Size(
        "count:Q",
        scale=alt.Scale(
            domain=domain_1
        ),
        legend=alt.Legend(
            title="Number of Alerts"
        )
    )
).properties(
    title="Top 10 Areas of 'Jam - Heavy Traffic' Alerts Number",
    height=300,
    width=300
).transform_filter(
    "datum.updated_type == 'Jam' & datum.updated_subtype == 'Heavy traffic'"
)

# Plot map
chart_map = alt.Chart(geo_data).mark_geoshape(
    fill="lightgray",
    stroke="white"
).project(
    type="equirectangular"
)


# Overlay the plots
chart_alerts_map = chart_map + chart_alerts

chart_alerts_map.show()
```

5. 

a. 

```{python}
print_file_contents("./top_alerts_map/app.py")
```

![](app1_5a.png){width=300 height=300}

There are 16 total combinations.

b. 

![](app1_5b.png){width=300 height=300}

c. 

![](app1_5c.png){width=300 height=300}

North east part of Portage Park neighborhood, which locate on north west part of Chicago.

d. 

Question: in which area road closure due to construction most commonly observed?

![](app1_5d.png){width=300 height=300}

Answer: West from loop, where large interstates (e.g. I-90/94) is running, which might be under maintenance or rehabilitation.

e. 

One idea is adding "roadType" column to the dashboard, as tooltip for each point. It would suggest in which road in the area the alerts happen, whether primary street or secondary/(standard) street etc, which is useful in answering the previous question I formulated above. The dataset dictionary suggests that the road types include even railroads or pedestrian, so it would be better to include this information to clarify the ratio of road types for each point in the dashboard.

# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}

1. 

a. 

Collapsing the dataset by the column *ts* would be not a good idea. We of course need to subset the dataset based on the information in *ts*, but we need to extract only hours from each value in the column, removing redundant information, such as year-month-date(irrelevant in our analysis), or minute and seconds (too detailed for input in UI, not requested by users), or "UTC" (All data in the column are UTC in this case).
    
b.  
```{python}
# Create "hour" column
df_waze_updated["hour"] = pd.to_datetime(
    df_waze_updated["ts"].replace("UTC", "")).dt.strftime("%H:00")

# Summarize the df by type, subtype, latitude, longitude and hour
df_top_alerts_map_byhour = df_waze_updated.groupby(
    ["updated_type", "updated_subtype", "longitude", "latitude", "hour"]
).size().reset_index().rename(
    columns={0: "count"}
)

# Group and filter top 10 from the sorted df by hour
df_top_alerts_map_byhour = df_top_alerts_map_byhour.sort_values(
    "count",
    ascending=False
).groupby(
    ["updated_type", "updated_subtype", "hour"]
).head(10).reset_index(drop=True)

df_top_alerts_map_byhour.to_csv(
    r"top_alerts_map_byhour\top_alerts_map_byhour.csv")
```

The level of aggregation is sets of type and subtype, and additionally, hour in this case.

```{python}
print(f"And the DataFrame has {len(df_top_alerts_map_byhour)} rows.")
```

c.

```{python}
# Create subset
df_top_alerts_map_byhour_chosen = df_top_alerts_map_byhour[(
    df_top_alerts_map_byhour["updated_type"] == "Jam"
) & (
    df_top_alerts_map_byhour["updated_subtype"] == "Heavy traffic"
) & (
    df_top_alerts_map_byhour["hour"].isin(["10:00", "16:00", "23:00"])
)]

# Set appropriate domain for chosen type and subtype
domain_2 = [
    df_top_alerts_map_byhour_chosen["count"].min(),
    df_top_alerts_map_byhour_chosen["count"].max()
]

# Plot the alerts by longitude and latitude for "Jam-Heavy Traffic" cases, for 3 timings
chart_alerts = alt.Chart().mark_point(
    color="firebrick",
    filled=True
).encode(
    longitude="longitude:Q",
    latitude="latitude:Q",
    size=alt.Size(
        "count:Q",
        scale=alt.Scale(
            domain=domain_2
        ),
        legend=alt.Legend(
            title="Number of Alerts"
        ))
).properties(
    height=150,
    width=150
)

chart_alerts_map_byhour = alt.layer(chart_map, chart_alerts).facet(
    data=df_top_alerts_map_byhour_chosen,
    column="hour:N"
).properties(
    title="Top 10 Areas of 'Jam - Heavy Traffic' Alerts Number"
)

chart_alerts_map_byhour.show()
```
    

2.

a. 

```{python}
print_file_contents("./top_alerts_map_byhour/app.py")
```

![](app2_2a.png){width=300 height=300}

b. 

![](app2_2b_1.png){width=300 height=300}

![](app2_2b_2.png){width=300 height=300}

![](app2_2b_3.png){width=300 height=300}

c. 

![](app2_2c_1.png){width=300 height=300}

![](app2_2c_2.png){width=300 height=300}

More during night hours. There seems no alerts before 11am, while there are frequent alerts at each night time. Even if there are other factor such as the difference in the amount of traffic between morning and night, the observation above would be strong enough to support my assumption.

# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}

1. 


a. 

Collapsing the dataset by range of hours would not be a good idea. If do so, the app need to aggregate the dataset internally for each time the slider was moved by the user, which is resource intensive.

b. 

```{python}
# Create base column for calculating range of hours
df_top_alerts_map_byhour["hour_dt"] = pd.to_datetime(
    df_top_alerts_map_byhour["hour"], format="%H:%M")
df_top_alerts_map_byhour["hour_dt"] = [x.time()
                                       for x in df_top_alerts_map_byhour["hour_dt"]]

# Create subset
df_top_alerts_map_byhour_range = df_top_alerts_map_byhour[(
    df_top_alerts_map_byhour["updated_type"] == "Jam"
) & (
    df_top_alerts_map_byhour["updated_subtype"] == "Heavy traffic"
) & (
    df_top_alerts_map_byhour["hour_dt"].between(time(6, 0), time(9, 0))
)].drop("hour_dt", axis=1).head(10)

# Set appropriate domain for chosen type and subtype
domain_3 = [
    df_top_alerts_map_byhour_range["count"].min(),
    df_top_alerts_map_byhour_range["count"].max()
]

# Plot the alerts by longitude and latitude for "Jam-Heavy Traffic" cases, for 3 timings
chart_alerts = alt.Chart(df_top_alerts_map_byhour_range).mark_point(
    color="firebrick",
    filled=True
).encode(
    longitude="longitude:Q",
    latitude="latitude:Q",
    size=alt.Size(
        "count:Q",
        scale=alt.Scale(
            domain=domain_3
        ),
        legend=alt.Legend(
            title="Number of Alerts"
        ))
).properties(
    height=200,
    width=200
)

chart_alerts_map_byhour_range = alt.layer(chart_map, chart_alerts).properties(
    title="Top 10 Areas of 'Jam - Heavy Traffic' Alerts Number between 6AM and 9AM"
)

chart_alerts_map_byhour_range.show()
```
<!--Attribution: parameter "format" refering to Perplexity (https://www.perplexity.ai/search/from-shiny-import-app-render-u-Tt7cqT5WTmeh0z2CLZtU0A)-->
<!--Attribution: Method ".time()" referring to Perplexity (https://www.perplexity.ai/search/how-to-make-date-time-object-w-bkYUZPR5R.6jqOOnOhDIzw)-->
<!--Attribution: Method ".between()" referring to Perplexity (https://www.perplexity.ai/search/df-top-alerts-maps-byhour-hour-vtZei4OvTCCm.fD0riMcVg)-->

2. 

a. 

```{python}
print_file_contents("./top_alerts_map_byhour_sliderrange/app.py")
```

![](app3_2a.png){width=300 height=300}

b. 
    
![](app3_2b.png){width=300 height=300}

3. 

a. 
    
```{python}
print_file_contents("./top_alerts_map_byhour_sliderrange/app_switch.py")
```

![](app3_3a.png){width=300 height=300}

possible values: True, False

b. 

![](app3_3b_1.png){width=300 height=300}

![](app3_3b_2.png){width=300 height=300}

c. 

The same screenshots as in b.

d.
